# -*- coding: utf-8 -*-
"""model_train.ipynb의 사본

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1UIOTdH00cwLsQCy0KcezNerIsLOOOmTw
"""

# 전처리(색상, 사이즈)된 데이터 다운(16x16, 32x32)
# !gdown --id 1RjprBx8pAjdfPTIifW_xm9SNb0lECfzm -O wafer_processed.zip

# !gdown --id 1-1_r-2tCiz1GtUNO6NA76NvSgjo-L1J6 -O augmented_pkl.zip

# !unzip -qq wafer_processed.zip -d data

# !unzip -qq augmented_pkl.zip -d data

import pandas as pd
import numpy as np
import numpy
import matplotlib.pyplot as plt
import tensorflow as tf
import os
import keras
import random
from keras.models import Sequential, Model
from keras.layers import Input, Dense, Dropout, Activation, Flatten, Conv2D
from keras.layers import BatchNormalization, Reshape, MaxPooling2D, GlobalAveragePooling2D
from datetime import datetime
from sklearn.metrics import f1_score
from sklearn.metrics import f1_score, recall_score, precision_score
from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc
from keras.optimizers import Adam

class DataGenerator(keras.utils.Sequence):
    'Generate mini-batchs'
    def __init__(self, X, Y, batch_size=32,
                shuffle=True, augment=True,
                balanced=False, verbose=0):
        '''
        X: X List
        Y: Y List - One Hot Encoding 된 상태여야 함
        batch_size: minibatch size
        shuffle: 데이터 shuffle 여부
        augment: 데이터 augment 여부
        balanced: balanced True일 시 y값이 비슷한 빈도가 되도록 랜덤 초이스
        verbose: 디버깅용
        '''
        self.shuffle = shuffle
        self.augment = augment
        self.batch_size = batch_size
        self._X = X
        self._Y = Y
        self._verbose = verbose
        self.balanced = balanced
        if self.balanced:
            self.setup_y_map()
        self.on_epoch_end()

    def ratate_image(self, image):
        v = np.random.rand() # 0 ~ 1
        if v < 0.125:
            if self._verbose:
                print("Augment - rotate90")
            return np.rot90(image)
        elif v < 0.25:
            if self._verbose:
                print("Augment - rotate180")
            return np.rot90(image, 2)
        elif v < 0.375:
            if self._verbose:
                print('Augment - rotate270')
            return np.rot90(image, 3)
        elif v < 0.5:
            if self._verbose:
                print('Not Augment')
            return image
        elif v < 0.625:
            if self._verbose:
                print("Augment - rotate90 & Flip left right")
            return np.fliplr(np.rot90(image))
        elif v < 0.75:
            if self._verbose:
                print("Augment - rotate180 & Flip left right")
            return np.fliplr(np.rot90(image, 2))
        elif v < 0.875:
            if self._verbose:
                print('Augment - rotate270 & Flip left right')
            return np.fliplr(np.rot90(image, 3))
        else:
            if self._verbose:
                print('Augment -  Flip left right')
            return np.fliplr(image)

    def augment_x(self, x):
        return self.ratate_image(x)

    def __len__(self):
        return int(np.ceil(len(self._X) / self.batch_size))

    def __getitem__(self, index):
        batch_X = None
        batch_Y = None
        if self.balanced:
            '''y 별로 비슷한 갯수가 되도록 Random Choice한다.'''
            batch_X = np.zeros((self.batch_size, *self._X.shape[1:]), dtype=self._X.dtype)
            batch_Y = np.zeros((self.batch_size, *self._Y.shape[1:]), dtype=self._Y.dtype)
            for idx in range(self.batch_size):
                y = random.choice(list(self._y_map.keys()))
                selected_idx = random.choice(self._y_map[y])
                batch_X[idx, :] = self._X[selected_idx, :]
                batch_Y[idx, :] = self._Y[selected_idx, :]
        else:
            indexes = self.indexes[index * self.batch_size : (index+1) * self.batch_size]
            batch_X = self._X[indexes]
            batch_Y = self._Y[indexes]

        if self.augment:
            batch_X = batch_X.copy()
            for row in range(len(batch_X)):
                batch_X[row] = self.augment_x(batch_X[row])

        return batch_X, batch_Y

    def setup_y_map(self):
        if self.balanced == False:
            return
        self._y_map = {}
        for idx in range(len(self._Y)):
            y = np.argmax(self._Y[idx])
            if y not in self._y_map:
                self._y_map[y] = []
            self._y_map[y].append(idx)

    def on_epoch_end(self):
        self.indexes = np.arange(len(self._X))
        if self.balanced == False and self.shuffle == True:
            np.random.shuffle(self.indexes)

class OneHotHelper:
    '일반 형태 <-> One Hot 형태'
    def __init__(self, labels=[]):
        self._labels = labels

    @property
    def labels(self):
        return self._labels

    @property
    def num_labels(self):
        return len(self._labels)

    def transform(self, normal_form):
        result = numpy.zeros((len(normal_form), self.num_labels), dtype=int)

        for row in range(len(normal_form)):
            value = normal_form[row]
            idx = self.labels.index(value)
            result[row, idx] = 1
        return result

    def recover(self, onehot_form):
        result = list()

        for row in range(len(onehot_form)):
            onehot = onehot_form[row]
            idx = numpy.argmax(onehot)
            result.append(self.labels[idx])
        return np.array(result)

def test_onehot():
    labels = ['Hi', 'Hello', 'World']
    helper = OneHotHelper(labels)
    result = helper.transform(['Hello', 'Hi', 'World', 'Hello'])
    print(result)
    recovered = helper.recover(result)
    print(recovered)

    labels = [0, 1, 2, 3, 4, 5, 6]
    helper = OneHotHelper(labels)
    result = helper.transform([6, 1, 2, 4, 4])
    print(result)
    recovered = helper.recover(result)
    print(recovered)

# test_onehot()


def train_model(model, name=None, image_size=32, early_stopping_min=1, epochs_min=20, path=None, balanced=True, augment=False):
    """
    model: 테스트할 모델 - input_shape는 (image_size, image_size, 3)와 일치해야 함)
    image_size: 사용할 웨이퍼 이미지 크기 16 혹은 32
    early_stopping_min: 이 시간(분) 동안 성능(validation metric인 macro f1-score 향상이 없다면 학습 종료)
    epochs_min: 학습할 최대 시간(분)
    path: 저장 경로(None일 시 현재 시간 기준으로 자동 생성)
    """
    now = datetime.now()
    time_str = now.strftime("%Y%m%d_%H%M%S")
    NAME = name
    if NAME is None:
        NAME = 'model_' + time_str
    IMAGE_SIZE = image_size   # 32, 16
    EARLY_STOPPING_MIN = early_stopping_min
    EPOCHS_MIN = epochs_min
    BALANCED = False # True이면 train batch 뽑을 때 y가 골고루 나오도록 한다.
    PATH = path
    if PATH is None:
        PATH = 'model_' + time_str

    def read_xy(pickle_path):
        df = pd.read_pickle(pickle_path)
        X = np.array(list(df['waferMap'].values), dtype=np.float32)
        Y = df['failureNum'].values
        return X, Y

    # x_train, y_train = read_xy('data/wafer_train_{}.pkl'.format(IMAGE_SIZE))
    x_train, y_train = None, None
    if balanced:
        x_train, y_train = read_xy('data/augmented.pkl')
    else:
        x_train, y_train = read_xy('data/wafer_train_32.pkl')

    print('x_train.shape:', x_train.shape)
    print('y_train.shape:', y_train.shape)

    x_valid, y_valid = read_xy('data/wafer_valid_{}.pkl'.format(IMAGE_SIZE))

    print('valid_X.shape:', x_valid.shape)
    print('valid_Y.shape:', y_valid.shape)

    x_test, y_test = read_xy('data/wafer_test_{}.pkl'.format(IMAGE_SIZE))


    onehot_helper = OneHotHelper(labels=[0, 1, 2, 3, 4, 5, 6, 7, 8])
    y_train_onehot = onehot_helper.transform(y_train)
    y_valid_onehot = onehot_helper.transform(y_valid)

    model.summary()

    dataset = DataGenerator(x_train, y_train_onehot, batch_size=32, shuffle=True, augment=augment)

    best_f1 = 0.0
    epoch_start = datetime.now()
    earlystop_start = datetime.now()
    model_save_path = os.path.join(PATH, NAME + '.h5')
    history_accum = {k:[] for k in ['loss', 'accuracy', 'val_loss', 'val_accuracy', 'f1_score']}


    while True:
        history = model.fit_generator(dataset)
        y_pred_soft = model.predict(x_valid)
        y_pred = np.array(onehot_helper.recover(y_pred_soft))
        f1 = f1_score(y_valid, y_pred, average='macro')
        print('f1:', f1)
        for k in history.history:
            history_accum[k] += history.history[k]
        history_accum['f1_score'] += [f1]
        if best_f1 < f1:
            best_f1 = f1
            earlystop_start = datetime.now()
            print('Best f1 changed:', best_f1)
            os.makedirs(PATH, exist_ok=True)
            model.save(model_save_path)
        elapsed_min = ((datetime.now() - epoch_start).total_seconds()) / 60.
        if elapsed_min >= EPOCHS_MIN:
            print('Epoch stopping')
            break
        earlystop_min = ((datetime.now() - earlystop_start).total_seconds()) / 60.
        if earlystop_min >= EARLY_STOPPING_MIN:
            print('Early stopping')
            break

    model = keras.models.load_model(model_save_path)

    y_pred_soft = model.predict(x_valid)
    y_pred = np.array(onehot_helper.recover(y_pred_soft))


    # 결과 기록
    os.makedirs(PATH, exist_ok=True)
    with open(os.path.join(PATH, 'result.txt'), 'w') as f:
        f1_res = f1_score(y_valid, y_pred, average='macro')
        f.write(str(f1_res))
        f.write('\n')
        print(f1_res)

        report = classification_report(y_valid, y_pred)
        print(report)
        f.write(str(report))
        f.write('\n')
        conf = confusion_matrix(y_valid, y_pred)
        print(conf)
        f.write(str(conf))
        f.write('\n')

        f.write(str(history_accum))
        f.write('\n')
        print(history_accum)

    y_test_soft = model.predict(x_test)
    y_test = np.array(onehot_helper.recover(y_test_soft))

    pd.DataFrame({
        'failureNum': y_test
    }).to_pickle(os.path.join(PATH, 'y_test_pred.pkl'))

# 테스트해 볼 모델들
def build_model1(image_size=32, learning_rate=0.001):
    input_shape = (image_size, image_size, 3)
    outputs = 9
    model = Sequential([
      Conv2D(16, 3, padding='same', activation='relu', input_shape=input_shape),
      MaxPooling2D(),
      Conv2D(32, 3, padding='same', activation='relu'),
      MaxPooling2D(),
      Conv2D(64, 3, padding='same', activation='relu'),
      MaxPooling2D(),
      Dropout(0.2),
      Flatten(),
      Dense(128, activation='relu'),
      Dense(outputs, activation='softmax')
    ])

    opt = Adam(lr=learning_rate)
    model.compile(loss = 'categorical_crossentropy', optimizer=opt, metrics=['accuracy'])

    return model

def build_model2(image_size=32, learning_rate=0.001):
    input_shape = (image_size, image_size, 3)
    outputs = 9
    model = Sequential([
      Conv2D(32, 3, padding='same', activation='relu', input_shape=input_shape),
      Conv2D(32, 3, padding='same', activation='relu'),
      MaxPooling2D(),
      Conv2D(32, 3, padding='same', activation='relu'),
      Conv2D(32, 3, padding='same', activation='relu'),
      MaxPooling2D(),
      Conv2D(32, 3, padding='same', activation='relu'),
      Conv2D(16, 3, padding='same', activation='relu'),
      MaxPooling2D(),
      Dropout(0.2),
      Flatten(),
      Dense(128, activation='relu'),
      Dense(outputs, activation='softmax')
    ])

    opt = Adam(lr=learning_rate)
    model.compile(loss = 'categorical_crossentropy', optimizer=opt, metrics=['accuracy'])

    return model

def build_model4(image_size=32, learning_rate=0.001):
    input_shape = (image_size, image_size, 3)
    outputs = 9
    model = Sequential([
      Conv2D(32, 3, padding='same', activation='relu', input_shape=input_shape),
      Conv2D(32, 3, padding='same', activation='relu'),
      MaxPooling2D(),
      Conv2D(16, 3, padding='same', activation='relu'),
      Conv2D(16, 3, padding='same', activation='relu'),
      MaxPooling2D(),
      Conv2D(8, 3, padding='same', activation='relu'),
      Conv2D(8, 3, padding='same', activation='relu'),
      MaxPooling2D(),
      Dropout(0.2),
      Flatten(),
      Dense(128, activation='relu'),
      Dense(outputs, activation='softmax')
    ])

    opt = Adam(lr=learning_rate)
    model.compile(loss = 'categorical_crossentropy', optimizer=opt, metrics=['accuracy'])

    return model

def build_SV_GAP(learning_rate=0.001, ksize=3, drop_rate=0.5, nlayers=3, node=512):
    input_shape = (32, 32, 3)
    outputs = 9
    layers = [
      Conv2D(64, (ksize, ksize), padding='same', activation='relu', input_shape=input_shape),
      Conv2D(64, (ksize, ksize), padding='same', activation='relu'),
      MaxPooling2D(pool_size=(2, 2), strides=(2, 2)),
      Conv2D(128, (ksize, ksize), padding='same', activation='relu'),
      Conv2D(128, (ksize, ksize), padding='same', activation='relu'),
    ]
    if nlayers >= 3:
        layers += [
            MaxPooling2D(pool_size=(2, 2), strides=(2, 2)),
            Conv2D(256, (ksize, ksize), padding='same', activation='relu'),
            Conv2D(256, (ksize, ksize), padding='same', activation='relu'),
        ]
    if nlayers >= 4:
        layers += [
            MaxPooling2D(pool_size=(2, 2), strides=(2, 2)),
            Conv2D(512, (ksize, ksize), padding='same', activation='relu'),
            Conv2D(512, (ksize, ksize), padding='same', activation='relu'),
        ]


    layers += [
      GlobalAveragePooling2D(),
      Dense(node, activation='relu'),
      Dropout(drop_rate),
      Dense(node, activation='relu'),
      Dropout(drop_rate),
      Dense(outputs, activation='softmax')
    ]

    model = Sequential(layers)
    opt = Adam(lr=learning_rate)
    model.compile(loss = 'categorical_crossentropy', optimizer=opt, metrics=['accuracy'])
    return model

def build_SV_GAP2(learning_rate=0.001, ksize=3, drop_rate=0.5, nlayers=3, node=512):
    input_shape = (32, 32, 3)
    outputs = 9
    layers = [
      Conv2D(128, (ksize, ksize), padding='same', activation='relu', input_shape=input_shape),
      Conv2D(128, (ksize, ksize), padding='same', activation='relu'),
      MaxPooling2D(pool_size=(2, 2), strides=(2, 2)),
      Conv2D(256, (ksize, ksize), padding='same', activation='relu'),
      Conv2D(256, (ksize, ksize), padding='same', activation='relu'),
      GlobalAveragePooling2D(),
      Dense(node, activation='relu'),
      Dropout(drop_rate),
      Dense(node, activation='relu'),
      Dropout(drop_rate),
      Dense(outputs, activation='softmax')
    ]

    model = Sequential(layers)
    opt = Adam(lr=learning_rate)
    model.compile(loss = 'categorical_crossentropy', optimizer=opt, metrics=['accuracy'])
    return model

# model: 테스트할 모델
# name: 모델 이름
# image_size: 사용할 wafer image size (32, 16 중 택)
# early_stopping_min: 주어진 시간(분) 동안 성능 개선 없을 시 종료
# epochs_min: 최대 학습 시간(분)
grid_lr = [0.0004, 0.00035, 0.00045]
grid_dropout = [0.20, 0.25, 0.15]
grid_kernel = [3]
grid_layers = [2]
grid_nodes = [256, 512, 1024]
grid_aug = [True, False]
grid_bal = [True, False]

import os

from keras import backend as K
import tensorflow as tf

strategy = tf.distribute.MirroredStrategy()
print('Number of devices: {}'.format(strategy.num_replicas_in_sync))

# Open a strategy scope.
with tf.device('/device:GPU:2'):
    for i in range(100):
        lr = np.random.choice(grid_lr)
        drop_rate = np.random.choice(grid_dropout)
        kernel = np.random.choice(grid_kernel)
        nlayers = np.random.choice(grid_layers)
        node = np.random.choice(grid_nodes)
        aug = np.random.choice(grid_aug)
        bal = np.random.choice(grid_bal)

        path = 'model_SV_GAP2_lr{}_dr{}_ks{}_ly{}_nd{}_aug{}_bal{}'.format(
                    lr, drop_rate, kernel, nlayers, node, aug, bal
        )
        if os.path.exists(path):
            continue
        model = build_SV_GAP2(learning_rate=lr, ksize=kernel, drop_rate=drop_rate,
        nlayers=nlayers, node=node)
        train_model(model=model, name=path, image_size=32, early_stopping_min=10,
        epochs_min=60,path=path, augment=aug, balanced=bal)

# model_xxxxx 폴더에 다음 파일들이 생성됨(Train 중에서 validation data에 대해 macro f1-score가 가장 높게 나왔을 때를 저장함)
# - model4.h5: Model 자체를 저장한 파일
# - result.txt: 결과 파일
# - y_test_pred.pkl: 제출용 test 파일을 예측한 결과

# 결과 다운로드 위해 압축
# !zip model_to_download.zip -r model_20201119_063030
